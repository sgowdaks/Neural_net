{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ebeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2LCOYbCFfJ3E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, num_words, emb_dim, num_y, embeds=None):\n",
        "    super().__init__()\n",
        "    print(num_words)\n",
        "    print(emb_dim)\n",
        "    print(num_y)\n",
        "    self.emb = nn.Embedding(num_words, emb_dim)\n",
        "    if embeds is not None:\n",
        "      self.emb.weight = nn.Parameter(torch.Tensor(embeds))\n",
        "    self.linear = nn.Linear(emb_dim, num_y)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, text):\n",
        "    embeds = torch.mean(self.emb(text), dim=0)\n",
        "    print(embeds)\n",
        "    return self.sigmoid(self.linear(embeds))"
      ],
      "metadata": {
        "id": "tPBD0SpwfKm6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocab(text):\n",
        "  word_to_ix = {}\n",
        "  for sent, label in text:\n",
        "    for word in sent.split():\n",
        "      word_to_ix.setdefault(word, len(word_to_ix))\n",
        "  # print(word_to_ix)\n",
        "  return word_to_ix\n"
      ],
      "metadata": {
        "id": "4wwfgmTxgA1D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [(\"I am doing good\", 1), (\"this is not right\", 0)]\n",
        "tok_to_ix = load_vocab(train_data)\n",
        "print(tok_to_ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXZMPJIsh8iV",
        "outputId": "f6544fa8-3fbb-418b-83d2-b894e82c8fb6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': 0, 'am': 1, 'doing': 2, 'good': 3, 'this': 4, 'is': 5, 'not': 6, 'right': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 1\n",
        "learning_rate = 0.0001\n",
        "emb_dim = 50\n",
        "embeds = None\n",
        "\n",
        "# embeds = api.load('glove-twitter-25').vectors # Change to 'word2vec-google-news-300' for word2vec or 'glove-wiki-gigaword-100' for Wikipedia-trained Glove\n",
        "# emb_dim = embeds.shape[1]\n",
        "\n",
        "model = Net(len(tok_to_ix), emb_dim, num_classes, embeds)\n",
        "print(model)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.BCELoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKwEsITXiCxn",
        "outputId": "24f2a0e0-4e4f-41de-cdcd-3f37bc78b26d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "50\n",
            "1\n",
            "Net(\n",
            "  (emb): Embedding(8, 50)\n",
            "  (linear): Linear(in_features=50, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "for epoch in range(n_epochs):\n",
        "  model.train()\n",
        "  for text, label in train_data:\n",
        "    print(text)\n",
        "    x = [tok_to_ix[tok] for tok in text.split()]\n",
        "    x_train_tensor = torch.LongTensor(x)\n",
        "    y_train_tensor = torch.Tensor([label])\n",
        "    pred_y = model(x_train_tensor)\n",
        "    loss = loss_fn(pred_y, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  print(\"\\nEpoch:\", epoch)\n",
        "  print(\"Training loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlXXHMItiGhM",
        "outputId": "92fb387d-1874-47db-9e5f-b650dfecbec5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0229,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1022,  0.9906, -1.0238,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8932, -0.0604, -0.4768, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5630, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3432,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4743,  0.4746, -0.3258,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 0\n",
            "Training loss: 0.7565340995788574\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0229,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1022,  0.9906, -1.0238,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8932, -0.0604, -0.4768, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5630, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3432,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4743,  0.4746, -0.3258,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 1\n",
            "Training loss: 0.7562804818153381\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0229,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1022,  0.9906, -1.0238,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8932, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3432,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 2\n",
            "Training loss: 0.7560269236564636\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0230,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1022,  0.9906, -1.0239,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8932, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3432,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 3\n",
            "Training loss: 0.7557734251022339\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0230,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1022,  0.9906, -1.0239,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8932, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3432,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 4\n",
            "Training loss: 0.7555200457572937\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0230,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1022,  0.9906, -1.0239,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8933, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3432,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 5\n",
            "Training loss: 0.7552667856216431\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0230,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1023,  0.9906, -1.0239,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8933, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3432,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 6\n",
            "Training loss: 0.755013644695282\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0230,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1023,  0.9906, -1.0239,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8933, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3431,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 7\n",
            "Training loss: 0.7547606825828552\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0230,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1023,  0.9906, -1.0239,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8933, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2562,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3431,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 8\n",
            "Training loss: 0.7545077800750732\n",
            "I am doing good\n",
            "tensor([-0.1267, -0.3910, -0.0230,  0.4649, -0.1008,  0.2766, -0.2146, -0.5575,\n",
            "         0.6730,  0.4272, -0.0530,  0.0225,  0.5759, -0.5417, -0.0534, -0.3583,\n",
            "        -0.4070, -0.3552,  0.4619, -0.2058, -0.8188,  0.9629, -0.3005, -0.9509,\n",
            "         0.1333,  0.1298,  0.2234, -0.1023,  0.9906, -1.0239,  0.2084,  0.2539,\n",
            "         0.0057,  0.9154,  0.8892, -0.2244,  0.3936,  0.5331,  0.1475,  0.2159,\n",
            "         0.1005, -0.4759,  0.0810,  0.8933, -0.0604, -0.4769, -0.5771, -0.6262,\n",
            "        -0.0260, -0.3744], grad_fn=<MeanBackward1>)\n",
            "this is not right\n",
            "tensor([-0.5629, -1.1594,  0.2563,  0.4998,  0.4587, -0.4231, -0.3306,  0.1650,\n",
            "        -0.7200, -0.3271, -0.8323, -0.5915, -0.1571,  0.4647, -0.5104,  0.3359,\n",
            "         0.3024, -0.1218,  0.2846,  0.0443, -0.4409, -0.2516, -0.2544,  0.2513,\n",
            "        -0.1352,  0.2772,  0.4354,  0.5196, -0.3431,  0.2859,  0.4212, -0.2957,\n",
            "         0.1819, -0.0728,  0.1215, -0.1898,  0.4586, -0.1297, -0.6502,  0.1970,\n",
            "        -0.4742,  0.4746, -0.3259,  0.2956, -0.1079, -0.3788, -0.6113, -0.2852,\n",
            "        -0.0110,  0.3809], grad_fn=<MeanBackward1>)\n",
            "\n",
            "Epoch: 9\n",
            "Training loss: 0.754254937171936\n"
          ]
        }
      ]
    }
  ]
}